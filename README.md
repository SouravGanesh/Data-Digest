ğ——ğ—®ğ˜ğ—® ğ—˜ğ—»ğ—´ğ—¶ğ—»ğ—²ğ—²ğ—¿ğ—¶ğ—»ğ—´ ğ—§ğ—²ğ—¿ğ—ºğ˜€ ğ—¬ğ—¼ğ˜‚ ğ—¡ğ—²ğ—²ğ—± ğ˜ğ—¼ ğ—ğ—»ğ—¼ğ˜„!

If you're into data engineering, knowing these terms will help you work with data storage, moving data around, and making sense of it all.

âš™ ğ——ğ—®ğ˜ğ—® ğ—£ğ—¶ğ—½ğ—²ğ—¹ğ—¶ğ—»ğ—²: An automated process that moves and prepares data.
ğŸ’¾ ğ——ğ—®ğ˜ğ—®ğ—¯ğ—®ğ˜€ğ—² [Link to Section 2](#ğ——ğ—®ğ˜ğ—®ğ—¯ğ—®ğ˜€ğ—²): An organized collection of data for easy access.

ğŸ“‹ ğ—¦ğ—°ğ—µğ—²ğ—ºğ—®: The blueprint defining a database's structure.
ğŸ’¡ ğ—§ğ—®ğ—¯ğ—¹ğ—²: A structured grid containing related data points.

ğŸ  ğ——ğ—®ğ˜ğ—® ğ—ªğ—®ğ—¿ğ—²ğ—µğ—¼ğ˜‚ğ˜€ğ—²: A central hub for integrated data analysis.
â¤µï¸ ğ—˜ğ—§ğ—Ÿ: Extract, Transform, Load - The traditional way to extract, clean, and load data.
â¤´ï¸ ğ—˜ğ—Ÿğ—§: Extract, Load, Transform - The modern approach of loading data first, then transforming it.

ğŸï¸ ğ——ğ—®ğ˜ğ—® ğ—Ÿğ—®ğ—¸ğ—²: Massive storage for raw, unorganized data.
â±ï¸ ğ—•ğ—®ğ˜ğ—°ğ—µ ğ—£ğ—¿ğ—¼ğ—°ğ—²ğ˜€ğ˜€ğ—¶ğ—»ğ—´: Processing data in large chunks at set times.
â±ï¸ ğ—¦ğ˜ğ—¿ğ—²ğ—®ğ—º ğ—£ğ—¿ğ—¼ğ—°ğ—²ğ˜€ğ˜€ğ—¶ğ—»ğ—´: Processing data in real-time as it arrives.

ğŸ“Š ğ——ğ—®ğ˜ğ—® ğ— ğ—®ğ—¿ğ˜: A specific slice of a data warehouse for a particular domain.
ğŸ” ğ——ğ—®ğ˜ğ—® ğ—¤ğ˜‚ğ—®ğ—¹ğ—¶ğ˜ğ˜†: Ensuring data accuracy, consistency, and reliability.
ğŸ•¸ï¸ ğ——ğ—®ğ˜ğ—® ğ— ğ—¼ğ—±ğ—²ğ—¹ğ—¶ğ—»ğ—´: Designing the logical structure and connections of data.

ğŸŒŠ ğ——ğ—®ğ˜ğ—® ğ—Ÿğ—®ğ—¸ğ—²ğ—µğ—¼ğ˜‚ğ˜€ğ—²: Combines the flexibility of a data lake with a data warehouse's structure.
ğŸ» ğ——ğ—®ğ˜ğ—® ğ—¢ğ—¿ğ—°ğ—µğ—²ğ˜€ğ˜ğ—¿ğ—®ğ˜ğ—¶ğ—¼ğ—»: Coordinating and managing complex data workflows.
ğŸ” ğ——ğ—®ğ˜ğ—® ğ—Ÿğ—¶ğ—»ğ—²ğ—®ğ—´ğ—²: Tracing a data's origin and journey through its use.

![Cheetsheet](https://github.com/SouravGanesh/Data-Digest/blob/057452f12e3934ad25bc5a46421500df41b482b1/images/de_terms.gif)

ğ——ğ—®ğ˜ğ—® ğ—œğ—»ğ˜ğ—²ğ—´ğ—¿ğ—®ğ˜ğ—¶ğ—¼ğ—» ğ—§ğ—²ğ—°ğ—µğ—»ğ—¶ğ—¾ğ˜‚ğ—²ğ˜€

- ğ——ğ—®ğ˜ğ—® ğ—˜ğ˜…ğ˜ğ—¿ğ—®ğ—°ğ˜ğ—¶ğ—¼ğ—»: Learn both full and incremental data extraction methods.

- ğ——ğ—®ğ˜ğ—® ğ—Ÿğ—¼ğ—®ğ—±ğ—¶ğ—»ğ—´:

 - ğ——ğ—®ğ˜ğ—®ğ—¯ğ—®ğ˜€ğ—²ğ˜€: Master the techniques of insert-only, insert-update, and comprehensive insert-update-delete operations.

 - ğ—™ğ—¶ğ—¹ğ—²ğ˜€: Understand how to replace files or append data within a folder.


ğ——ğ—®ğ˜ğ—® ğ—§ğ—¿ğ—®ğ—»ğ˜€ğ—³ğ—¼ğ—¿ğ—ºğ—®ğ˜ğ—¶ğ—¼ğ—» ğ—¦ğ˜ğ—¿ğ—®ğ˜ğ—²ğ—´ğ—¶ğ—²ğ˜€

- ğ——ğ—®ğ˜ğ—®ğ—™ğ—¿ğ—®ğ—ºğ—²ğ˜€: Acquire skills in manipulating CSV and Parquet file data with tools like Pandas and Polars.

- ğ—¦ğ—¤ğ—Ÿ: Enhance your ability to transform data within PostgreSQL databases using SQL. 

This includes executing complex aggregations with window functions, breaking down transformation logic with Common Table Expressions (CTEs), and applying transformations in open-source databases such as PostgreSQL.


ğ——ğ—®ğ˜ğ—® ğ—¢ğ—¿ğ—°ğ—µğ—²ğ˜€ğ˜ğ—¿ğ—®ğ˜ğ—¶ğ—¼ğ—» ğ—™ğ˜‚ğ—»ğ—±ğ—®ğ—ºğ—²ğ—»ğ˜ğ—®ğ—¹ğ˜€

- Develop the ability to create a Directed Acyclic Graph (DAG) using Python.

- Gain expertise in generating logs for monitoring code execution and incorporate logging into databases like PostgreSQL. Learn to trigger alerts for failed runs.

- Familiarize yourself with scheduling Python DAGs using cron expressions.


<a name="Database"></a>
## Database
1. ğ—¦ğ—½ğ—®ğ˜ğ—¶ğ—®ğ—¹ ğ——ğ—®ğ˜ğ—®ğ—¯ğ—®ğ˜€ğ—²ğ˜€: Handle spatial data, perfect for applications involving maps, GIS, and location-based services. Ideal for navigating a world of coordinates!

2. ğ—œğ—»-ğ— ğ—²ğ—ºğ—¼ğ—¿ğ˜† ğ——ğ—®ğ˜ğ—®ğ—¯ğ—®ğ˜€ğ—²ğ˜€: Lightning-fast data retrieval by storing everything in RAM. Perfect for applications where speed is of the essence.

3. ğ——ğ—¶ğ˜€ğ˜ğ—¿ğ—¶ğ—¯ğ˜‚ğ˜ğ—²ğ—± ğ——ğ—®ğ˜ğ—®ğ—¯ğ—®ğ˜€ğ—²ğ˜€: Spread your data across multiple servers, ensuring scalability, fault tolerance, and improved performance in a distributed environment.

4. ğ—¥ğ—²ğ—¹ğ—®ğ˜ğ—¶ğ—¼ğ—»ğ—®ğ—¹ ğ——ğ—®ğ˜ğ—®ğ—¯ğ—®ğ˜€ğ—²ğ˜€: Classic and structured, these databases use tables to organize and link data, adhering to the principles of relational algebra. Think SQL databases!

5. ğ—šğ—¿ğ—®ğ—½ğ—µ ğ——ğ—®ğ˜ğ—®ğ—¯ğ—®ğ˜€ğ—²ğ˜€: Unravel complex relationships in data through nodes and edges, making them perfect for social networks, fraud detection, and network analysis.

6. ğ—§ğ—¶ğ—ºğ—²-ğ—¦ğ—²ğ—¿ğ—¶ğ—²ğ˜€ ğ——ğ—®ğ˜ğ—®ğ—¯ğ—®ğ˜€ğ—²ğ˜€: Tailored for managing time-stamped data, crucial for applications involving IoT, financial trading, and system monitoring.

7. ğ—¢ğ—¯ğ—·ğ—²ğ—°ğ˜-ğ—¢ğ—¿ğ—¶ğ—²ğ—»ğ˜ğ—²ğ—± ğ——ğ—®ğ˜ğ—®ğ—¯ğ—®ğ˜€ğ—²ğ˜€: Like Relational Databases, but they store data in objects, mirroring real-world entities. Perfect for systems where data is naturally object-oriented.

8. ğ—¡ğ—¼ğ—¦ğ—¤ğ—Ÿ ğ——ğ—®ğ˜ğ—®ğ—¯ğ—®ğ˜€ğ—²ğ˜€: Perfect for unstructured or semi-structured data, allowing you to manage a variety of data types efficiently. They offer horizontal scalability, high availability, and flexibility in schema design.

9. ğ—©ğ—²ğ—°ğ˜ğ—¼ğ—¿ ğ——ğ—®ğ˜ğ—®ğ—¯ğ—®ğ˜€ğ—²ğ˜€: Harness the power of vectors to handle complex mathematical computations efficiently, ideal for applications like machine learning and data analytics.

Understanding their differences is not just about knowledge, it's about making informed decisions for efficient and effective data handling.

![Cheetsheet](https://github.com/SouravGanesh/Data-Digest/blob/fa777e859028b70d88e16a224670ea8d6811eecb/images/database.png)




ğŸ­. ğ—Ÿğ—¶ğ—»ğ—²ğ—®ğ—¿ ğ—¥ğ—²ğ—´ğ—¿ğ—²ğ˜€ğ˜€ğ—¶ğ—¼ğ—»: Imagine you want to predict the price of a house based on its size. Linear regression helps you find the straight-line relationship between size and price.

ğŸ®. ğ—Ÿğ—¼ğ—´ğ—¶ğ˜€ğ˜ğ—¶ğ—° ğ—¥ğ—²ğ—´ğ—¿ğ—²ğ˜€ğ˜€ğ—¶ğ—¼ğ—»: Contrary to what the name suggests, this is actually for classification tasks. It helps you decide if an email is spam or not by calculating the probability.

ğŸ¯. ğ——ğ—²ğ—°ğ—¶ğ˜€ğ—¶ğ—¼ğ—» ğ—§ğ—¿ğ—²ğ—²ğ˜€: These are like flowcharts that lead you to a decision by asking a series of questions based on the data's features.

ğŸ°. ğ—¥ğ—®ğ—»ğ—±ğ—¼ğ—º ğ—™ğ—¼ğ—¿ğ—²ğ˜€ğ˜: This one builds a whole 'forest' of decision trees and merges them to get more accurate and stable predictions.

ğŸ±. ğ—¦ğ˜‚ğ—½ğ—½ğ—¼ğ—¿ğ˜ ğ—©ğ—²ğ—°ğ˜ğ—¼ğ—¿ ğ— ğ—®ğ—°ğ—µğ—¶ğ—»ğ—²ğ˜€ (ğ—¦ğ—©ğ— ): If your data points are apples and oranges, SVM finds the best line that separates the apples from the oranges.

ğŸ². ğ—-ğ—¡ğ—²ğ—®ğ—¿ğ—²ğ˜€ğ˜ ğ—¡ğ—²ğ—¶ğ—´ğ—µğ—¯ğ—¼ğ—¿ğ˜€ (ğ—ğ—¡ğ—¡): This algorithm looks at the closest data points, like neighbors, to decide the category of a new point.

ğŸ³. ğ—¡ğ—®ğ—¶ğ˜ƒğ—² ğ—•ğ—®ğ˜†ğ—²ğ˜€: It's based on probability and is really good for things like filtering spam or analyzing sentiment.

ğŸ´. ğ—šğ—¿ğ—®ğ—±ğ—¶ğ—²ğ—»ğ˜ ğ—•ğ—¼ğ—¼ğ˜€ğ˜ğ—¶ğ—»ğ—´ ğ— ğ—®ğ—°ğ—µğ—¶ğ—»ğ—²ğ˜€ (ğ—šğ—•ğ— ): This is like a smart assembly line, where each new machine corrects the mistakes of the previous one to improve results.

![ML Cheetsheet](images/ml_cheeetsheet.png)

ğ——ğ—®ğ˜ğ—® ğ— ğ—¼ğ—±ğ—²ğ—¹ğ—¶ğ—»ğ—´ - ğ—§ğ—µğ—² ğ—•ğ—¹ğ˜‚ğ—²ğ—½ğ—¿ğ—¶ğ—»ğ˜ ğ—³ğ—¼ğ—¿ ğ——ğ—®ğ˜ğ—® ğ—¦ğ˜‚ğ—°ğ—°ğ—²ğ˜€ğ˜€

Data Modeling is the art and science of creating a structured framework to handle the influx and storage of data. It's like the architectural blueprint of your data environment, ensuring efficiency, consistency, and scalability. In essence, it's your roadmap for data success.

ğŸ“˜ ğ—–ğ—¼ğ—¿ğ—² ğ—–ğ—¼ğ—»ğ—°ğ—²ğ—½ğ˜ğ˜€ ğ—¶ğ—» ğ—œğ—»ğ—³ğ—¼ğ—¿ğ—ºğ—®ğ˜ğ—¶ğ—¼ğ—» ğ— ğ—¼ğ—±ğ—²ğ—¹ğ—¶ğ—»ğ—´
ğ—¦ğ—°ğ—µğ—²ğ—ºğ—®: This acts as the guidebook for your database, defining how information is structured and establishing the connections between different data fragments.
ğ—™ğ—®ğ—°ğ˜ ğ—§ğ—®ğ—¯ğ—¹ğ—²: This serves as the central storage ğŸ¦ for all the essential metrics and performance indicators. It's the nucleus of your data cosmos.
ğ——ğ—¶ğ—ºğ—²ğ—»ğ˜€ğ—¶ğ—¼ğ—» ğ—§ğ—®ğ—¯ğ—¹ğ—²ğ˜€: Encircling your fact table, these tables contain illustrative, textual, or categorial details. Consider them as the backdrop to your metrics.

ğŸŒŸ ğ—§ğ˜„ğ—¼ ğ— ğ—®ğ—¶ğ—» ğ—§ğ˜†ğ—½ğ—²ğ˜€ ğ—¼ğ—³ ğ——ğ—®ğ˜ğ—® ğ——ğ—²ğ˜€ğ—¶ğ—´ğ—» ğ—¶ğ—» ğ——ğ—®ğ˜ğ—® ğ—ªğ—®ğ—¿ğ—²ğ—µğ—¼ğ˜‚ğ˜€ğ—²ğ˜€

1. ğ—¦ğ˜ğ—®ğ—¿ ğ—¦ğ—°ğ—µğ—²ğ—ºğ—® ğŸŒŸ
ğ—ªğ—µğ—®ğ˜ ğ—¶ğ˜€ ğ—¶ğ˜?: In a Star Schema, the fact table takes its central position, directly linked to various dimension tables.
ğ—£ğ—¿ğ—¼ğ˜€: Extremely user-friendly and ensures rapid query performances.
ğ—–ğ—¼ğ—»ğ˜€: Potential data redundancy issues.

2. ğ—¦ğ—»ğ—¼ğ˜„ğ—³ğ—¹ğ—®ğ—¸ğ—² ğ—¦ğ—°ğ—µğ—²ğ—ºğ—® â„ï¸
ğ—ªğ—µğ—®ğ˜ ğ—¶ğ˜€ ğ—¶ğ˜?: The Snowflake Schema elevates it a notch by normalizing dimension tables into sub-dimensions, forming a hierarchy.
ğ—£ğ—¿ğ—¼ğ˜€: Highly normalized, minimizing data redundancy.
ğ—–ğ—¼ğ—»ğ˜€: More intricate queries and potential sluggishness.

ğ—¦ğ—¼, ğ—ªğ—µğ—¶ğ—°ğ—µ ğ—¢ğ—»ğ—² ğ˜ğ—¼ ğ—–ğ—µğ—¼ğ—¼ğ˜€ğ—²?
ğ—šğ—¼ ğ—¦ğ˜ğ—®ğ—¿ ğŸŒŸ: If you're after simplicity and quicker query outcomes.
ğ—£ğ—¶ğ—°ğ—¸ ğ—¦ğ—»ğ—¼ğ˜„ğ—³ğ—¹ğ—®ğ—¸ğ—² â„ï¸: If you're concerned about data storage and normalization.

![Cheetsheet](images/shema_cheeetsheet.png)


ğ——ğ—®ğ˜ğ—® ğ—Ÿğ—®ğ—¸ğ—² ğ˜ƒğ˜€ ğ——ğ—®ğ˜ğ—® ğ—ªğ—®ğ—¿ğ—²ğ—µğ—¼ğ˜‚ğ˜€ğ—²: ğ—ğ—²ğ˜† ğ——ğ—¶ğ—³ğ—³ğ—²ğ—¿ğ—²ğ—»ğ—°ğ—²ğ˜€ ğŸ‘¨â€ğŸ’»

When building a data analytics infrastructure, two technologies often come to mind - data lakes and data warehouses. But when should you choose one over the other? Here's a look at their key differences and strengths:

ğŸŒŠ ğ——ğ—®ğ˜ğ—® ğ—Ÿğ—®ğ—¸ğ—²ğ˜€: Flexible Storage and Analytics for Varied Data
Data lakes can store any type of data, both structured and unstructured, including text, images, video, audio, and more. With this flexibility, data lakes are ideal for machine learning, artificial intelligence, and advanced analytics on large volumes of varied data. Their scalability accommodates huge datasets. Data lakes allow you to store data now and decide how to use it later.

ğŸ  ğ——ğ—®ğ˜ğ—® ğ—ªğ—®ğ—¿ğ—²ğ—µğ—¼ğ˜‚ğ˜€ğ—²ğ˜€: Structured and Optimized for BI Workloads
Data warehouses store structured, optimized data that's been cleaned and processed for downstream analytics. Their relational structure and optimization makes them ideal for predefined workloads like business intelligence, reporting, and dashboards. But data warehouses lack flexibility for unstructured data and can't scale to massive datasets cost-effectively.

Overall, data lakes provide flexibility and scalability for exploratory analytics on varied data, while data warehouses are tailored for structured data and predefined workloads. Assess your use cases, data types and volumes, performance needs, and costs to choose the right technology for your needs.

![Cheetsheet](images/data_l&w_cheeetsheet.png)

ğ——ğ—®ğ˜ğ—®ğ—¯ğ—¿ğ—¶ğ—°ğ—¸ğ˜€ ğ˜ƒğ˜€. ğ—¦ğ—»ğ—¼ğ˜„ğ—³ğ—¹ğ—®ğ—¸ğ—²: ğ—” ğ—–ğ—¼ğ—ºğ—½ğ—®ğ—¿ğ—¶ğ˜€ğ—¼ğ—» ğŸ¥Šâ˜

With data volumes surging, picking the right cloud data warehouse is critical. Let's explore how leading options Databricks and Snowflake compare.

âš¡ ğ——ğ—®ğ˜ğ—®ğ—¯ğ—¿ğ—¶ğ—°ğ—¸ğ˜€:
- Unified analytics platform optimized for big data
- Leverages Spark for scalable distributed processing
- Supports real-time streaming analytics
- Easy integration with ML libraries like TensorFlow

â„ ğ—¦ğ—»ğ—¼ğ˜„ğ—³ğ—¹ğ—®ğ—¸ğ—²:
- Cloud-native data warehouse designed for flexibility
- Segregates storage and compute for scalability
- Fast performance for high-concurrency SQL analytics
- Virtual warehouses provide tunable resources

ğ—ğ—²ğ˜† ğ——ğ—¶ğ—³ğ—³ğ—²ğ—¿ğ—²ğ—»ğ—°ğ—²ğ˜€:
Databricks excels at big data workflows and advanced analytics while Snowflake powers SQL analytics and serves as a cloud data warehouse. Databricks offers better support for unstructured data while Snowflake specializes in structured data.

ğ—¨ğ˜€ğ—² ğ—–ğ—®ğ˜€ğ—²ğ˜€:
Databricks shines for ETL, machine learning, and real-time analytics. Snowflake is ideal for BI and data sharing use cases that require querying large datasets.

ğ—–ğ—¼ğ—»ğ—°ğ—¹ğ˜‚ğ˜€ğ—¶ğ—¼ğ—»:
Databricks and Snowflake both deliver robust analytics capabilities through different approaches. Understanding their respective strengths and aligning them to your use cases is key to choosing the right platform. Evaluating your requirements and data infrastructure will determine which solution fits best. With clear goals, you can leverage these technologies for impactful data insights!

![Cheetsheet](https://github.com/SouravGanesh/Data-Digest/blob/e874d49ddc0685dc7755ccc8a2fb0991286d0d8e/images/data_db%26sf_cheeetsheet.png)


The term 'Data Lake' might sound like tech jargon, but let's demystify it.

Introduced in the early 2010s, a Data Lake is a centralized repository designed to store vast amounts of raw data, irrespective of its source or format. 

Unlike traditional systems, it doesn't discriminate â€“ structured, semi-structured, or unstructured; it welcomes all types of data. 

ğ—§ğ—µğ—² ğ—¶ğ—±ğ—²ğ—®? 

To have a single source of truth, ready to be analyzed when needed.

Here's a step-by-step journey through a Data Lake:

1.ğ——ğ—®ğ˜ğ—® ğ—œğ—»ğ—´ğ—²ğ˜€ğ˜ğ—¶ğ—¼ğ—»:
Firstly, we collect data from varied sources:
- ğŸ“œ Logs: Diagnostic records.
- ğŸŒ Social Media: Capturing real-time sentiments.
- â˜ï¸ Cloud: A reservoir of scalable resources.
- ğŸ”„ ERP/CRM: The epicenter of your business operations.
- ğŸ“‚ Flat Files & ğŸ—ƒï¸ OLTP: Simplicity and real-time transactions.

2.ğ—˜ğ—§ğ—Ÿ ğ—£ğ—¿ğ—¼ğ—°ğ—²ğ˜€ğ˜€:
Once data enters the system, the ETL (Extract, Transform, Load) process kicks in. This is where data is cleansed, transformed, and made ready for analysis.

3.ğ—¦ğ˜ğ—¼ğ—¿ğ—®ğ—´ğ—² ğ—®ğ—»ğ—± ğ— ğ—®ğ—»ğ—®ğ—´ğ—²ğ—ºğ—²ğ—»ğ˜:
The Data Lake has distinct zones:
- ğŸ”¶ ğ—•ğ—¿ğ—¼ğ—»ğ˜‡ğ—² ğ—­ğ—¼ğ—»ğ—²: Raw data's initial resting place.
- ğŸ”· ğ—¦ğ—¶ğ—¹ğ˜ƒğ—²ğ—¿ ğ—­ğ—¼ğ—»ğ—²: The transformation stage where data is cleaned and structured.
- ğŸ¥‡ ğ—šğ—¼ğ—¹ğ—± ğ—­ğ—¼ğ—»ğ—²: Here, data is refined, trusted, and ready for consumption.

ğŸ›¡ï¸ ğ—¦ğ—²ğ—°ğ˜‚ğ—¿ğ—¶ğ˜ğ˜†: Ensuring data integrity and protection.
ğŸ” ğ—¦ğ—²ğ—®ğ—¿ğ—°ğ—µ: Efficiently find the data you need.
ğŸŒ ğ—šğ—¼ğ˜ƒğ—²ğ—¿ğ—» ğ—­ğ—¼ğ—»ğ—²: Where data management policies reside to maintain data quality and compliance.

ğŸ”½ ğ——ğ—®ğ˜ğ—® ğ—–ğ—¼ğ—»ğ˜€ğ˜‚ğ—ºğ—½ğ˜ğ—¶ğ—¼ğ—»: The final frontier where data meets its destiny â€“ be it in analytics, machine learning models, or business reports.

Remember, the essence of a Data Lake is not just in its storage but how effectively we can transform raw data into actionable insights. 

![Cheetsheet](https://github.com/SouravGanesh/Data-Digest/blob/4ce6a2dd3e09543757c52b77463f1712b97cee26/images/datalake.gif)

As data volumes grow, databases often struggle to keep up with performance demands. 

One powerful technique to scale databases is sharding - horizontally partitioning data across multiple databases.

Vertical Partitioning splits tables by columns, with different columns stored in separate tables. This allows frequently accessed columns to be separated for faster querying.

Horizontal Partitioning, in contrast, involves splitting a table across different databases by rows. Large tables are divided into smaller shards that can be queried in parallel.

ğ—ğ—²ğ˜†-ğ—¯ğ—®ğ˜€ğ—²ğ—± ğ˜€ğ—µğ—®ğ—¿ğ—±ğ—¶ğ—»ğ—´ determines the shard for each row based on the value of a "shard key" column. Records with shard key values in a certain range are stored together. 

ğ—¥ğ—®ğ—»ğ—´ğ—²-ğ—¯ğ—®ğ˜€ğ—²ğ—± ğ˜€ğ—µğ—®ğ—¿ğ—±ğ—¶ğ—»ğ—´ is a variation where ranges of shard key values are allocated to each shard. The infographic shows an example where products priced $120-150 are in one shard, $151-500 in another, and $501+ in a third shard. This enables fine-grained allocation of key ranges to shards based on value distribution.

ğ——ğ—¶ğ—¿ğ—²ğ—°ğ˜ğ—¼ğ—¿ğ˜†-ğ—¯ğ—®ğ˜€ğ—²ğ—± ğ˜€ğ—µğ—®ğ—¿ğ—±ğ—¶ğ—»ğ—´ adds a lookup table that maps a shard key to the database shard, allowing more flexibility than key-based sharding in allocating data to shards.

Sharding is a great way to achieve horizontal scaling and handle increasing data volumes. 
![Cheetsheet](https://github.com/SouravGanesh/Data-Digest/blob/317eb346833fef8ba4ebdbaf2dfa4b89d6f5a838/images/sharding.png)
